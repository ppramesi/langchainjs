## Using `.stream()`

import CodeBlock from "@theme/CodeBlock";

The easiest way to stream is to use the `.stream()` method. This returns an readable stream that you can also iterate over:

import StreamMethodExample from "@examples/models/llm/llm_streaming_stream_method.ts";

<CodeBlock language="typescript">{StreamMethodExample}</CodeBlock>

For models that do not support streaming, the entire response will be returned as a single chunk.

## Using a callback handler

You can also use a [`CallbackHandler`](https://github.com/hwchase17/langchainjs/blob/main/langchain/src/callbacks/base.ts) like so:

import StreamingExample from "@examples/models/llm/llm_streaming.ts";

<CodeBlock language="typescript">{StreamingExample}</CodeBlock>

We still have access to the end `LLMResult` if using `generate`. However, `token_usage` is not currently supported for streaming.
